{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "from scipy.signal import convolve2d, correlate2d\n",
    "from torch.nn.modules.module import Module \n",
    "from torch.nn.parameter import Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 732.5946044921875)\n",
      "(1, 676.2997436523438)\n",
      "(2, 683.893798828125)\n",
      "(3, 681.8519897460938)\n",
      "(4, 469.0211181640625)\n",
      "(5, 408.431396484375)\n",
      "(6, 678.7081298828125)\n",
      "(7, 677.68798828125)\n",
      "(8, 673.5112915039062)\n",
      "(9, 675.185302734375)\n",
      "(10, 668.5148315429688)\n",
      "(11, 206.9691619873047)\n",
      "(12, 671.1268920898438)\n",
      "(13, 165.8954315185547)\n",
      "(14, 668.1176147460938)\n",
      "(15, 666.1344604492188)\n",
      "(16, 663.5509643554688)\n",
      "(17, 92.0317153930664)\n",
      "(18, 592.6405639648438)\n",
      "(19, 640.1012573242188)\n",
      "(20, 649.8265991210938)\n",
      "(21, 532.0264892578125)\n",
      "(22, 73.52848052978516)\n",
      "(23, 595.9910888671875)\n",
      "(24, 445.9525146484375)\n",
      "(25, 608.4380493164062)\n",
      "(26, 591.6952514648438)\n",
      "(27, 506.4504699707031)\n",
      "(28, 540.6077880859375)\n",
      "(29, 292.7088317871094)\n",
      "(30, 131.9813690185547)\n",
      "(31, 377.5372314453125)\n",
      "(32, 234.17576599121094)\n",
      "(33, 381.8858337402344)\n",
      "(34, 350.0209045410156)\n",
      "(35, 194.21656799316406)\n",
      "(36, 265.5238342285156)\n",
      "(37, 243.4134063720703)\n",
      "(38, 155.14788818359375)\n",
      "(39, 208.06373596191406)\n",
      "(40, 147.27505493164062)\n",
      "(41, 216.2056884765625)\n",
      "(42, 155.80162048339844)\n",
      "(43, 103.7174301147461)\n",
      "(44, 137.28497314453125)\n",
      "(45, 166.87423706054688)\n",
      "(46, 90.07152557373047)\n",
      "(47, 119.27288818359375)\n",
      "(48, 96.12440490722656)\n",
      "(49, 60.40530014038086)\n",
      "(50, 102.43765258789062)\n",
      "(51, 82.5474624633789)\n",
      "(52, 85.8266372680664)\n",
      "(53, 76.96527099609375)\n",
      "(54, 43.12047576904297)\n",
      "(55, 213.70578002929688)\n",
      "(56, 83.85274505615234)\n",
      "(57, 68.80440521240234)\n",
      "(58, 638.7472534179688)\n",
      "(59, 187.33531188964844)\n",
      "(60, 192.24990844726562)\n",
      "(61, 406.0650634765625)\n",
      "(62, 392.7998046875)\n",
      "(63, 309.48876953125)\n",
      "(64, 359.66680908203125)\n",
      "(65, 352.2297668457031)\n",
      "(66, 335.97314453125)\n",
      "(67, 239.01454162597656)\n",
      "(68, 200.11953735351562)\n",
      "(69, 178.1610870361328)\n",
      "(70, 139.859375)\n",
      "(71, 107.46660614013672)\n",
      "(72, 216.6649627685547)\n",
      "(73, 169.21693420410156)\n",
      "(74, 147.32321166992188)\n",
      "(75, 88.50146484375)\n",
      "(76, 177.57754516601562)\n",
      "(77, 169.22317504882812)\n",
      "(78, 160.06163024902344)\n",
      "(79, 129.69834899902344)\n",
      "(80, 115.1114730834961)\n",
      "(81, 96.05242156982422)\n",
      "(82, 109.0448226928711)\n",
      "(83, 145.02505493164062)\n",
      "(84, 72.46684265136719)\n",
      "(85, 64.06218719482422)\n",
      "(86, 98.5942153930664)\n",
      "(87, 40.110137939453125)\n",
      "(88, 92.50859069824219)\n",
      "(89, 111.60597229003906)\n",
      "(90, 63.289764404296875)\n",
      "(91, 25.503883361816406)\n",
      "(92, 111.24899291992188)\n",
      "(93, 26.66996192932129)\n",
      "(94, 22.701881408691406)\n",
      "(95, 53.81124496459961)\n",
      "(96, 51.19115447998047)\n",
      "(97, 12.382237434387207)\n",
      "(98, 66.16940307617188)\n",
      "(99, 86.63554382324219)\n",
      "(100, 50.39500427246094)\n",
      "(101, 18.261825561523438)\n",
      "(102, 91.40045166015625)\n",
      "(103, 49.373390197753906)\n",
      "(104, 57.9687614440918)\n",
      "(105, 61.32011032104492)\n",
      "(106, 30.693754196166992)\n",
      "(107, 55.93862533569336)\n",
      "(108, 20.33493423461914)\n",
      "(109, 17.855154037475586)\n",
      "(110, 50.62332534790039)\n",
      "(111, 32.52718734741211)\n",
      "(112, 33.19335174560547)\n",
      "(113, 21.105712890625)\n",
      "(114, 22.137903213500977)\n",
      "(115, 12.793686866760254)\n",
      "(116, 21.75629234313965)\n",
      "(117, 19.295839309692383)\n",
      "(118, 8.986711502075195)\n",
      "(119, 27.29949188232422)\n",
      "(120, 8.697251319885254)\n",
      "(121, 23.645483016967773)\n",
      "(122, 22.142595291137695)\n",
      "(123, 14.822901725769043)\n",
      "(124, 10.529313087463379)\n",
      "(125, 7.360371112823486)\n",
      "(126, 8.6465425491333)\n",
      "(127, 25.07748031616211)\n",
      "(128, 11.403044700622559)\n",
      "(129, 23.71569061279297)\n",
      "(130, 19.90693473815918)\n",
      "(131, 26.993749618530273)\n",
      "(132, 19.794452667236328)\n",
      "(133, 12.606945037841797)\n",
      "(134, 9.158546447753906)\n",
      "(135, 11.642609596252441)\n",
      "(136, 58.56794738769531)\n",
      "(137, 10.668283462524414)\n",
      "(138, 13.56132698059082)\n",
      "(139, 15.833815574645996)\n",
      "(140, 54.36947250366211)\n",
      "(141, 101.93204498291016)\n",
      "(142, 15.025327682495117)\n",
      "(143, 19.96251678466797)\n",
      "(144, 35.338523864746094)\n",
      "(145, 38.501792907714844)\n",
      "(146, 34.59443664550781)\n",
      "(147, 15.992557525634766)\n",
      "(148, 17.622716903686523)\n",
      "(149, 12.6272554397583)\n",
      "(150, 12.068041801452637)\n",
      "(151, 18.50413703918457)\n",
      "(152, 10.978449821472168)\n",
      "(153, 44.47133255004883)\n",
      "(154, 6.416454792022705)\n",
      "(155, 6.807858943939209)\n",
      "(156, 9.66186809539795)\n",
      "(157, 15.951387405395508)\n",
      "(158, 13.113896369934082)\n",
      "(159, 15.264008522033691)\n",
      "(160, 5.678159713745117)\n",
      "(161, 7.245611667633057)\n",
      "(162, 9.72893238067627)\n",
      "(163, 29.62078285217285)\n",
      "(164, 12.568582534790039)\n",
      "(165, 6.192354679107666)\n",
      "(166, 5.245658874511719)\n",
      "(167, 9.13615608215332)\n",
      "(168, 9.286038398742676)\n",
      "(169, 7.105342388153076)\n",
      "(170, 11.61242961883545)\n",
      "(171, 4.988853454589844)\n",
      "(172, 3.534318208694458)\n",
      "(173, 3.2817773818969727)\n",
      "(174, 3.0607967376708984)\n",
      "(175, 25.496742248535156)\n",
      "(176, 11.101472854614258)\n",
      "(177, 5.121203422546387)\n",
      "(178, 2.667057991027832)\n",
      "(179, 13.779109001159668)\n",
      "(180, 2.496570348739624)\n",
      "(181, 27.42699432373047)\n",
      "(182, 4.813145160675049)\n",
      "(183, 17.15497398376465)\n",
      "(184, 9.107301712036133)\n",
      "(185, 17.8728084564209)\n",
      "(186, 20.460742950439453)\n",
      "(187, 18.4125919342041)\n",
      "(188, 11.105973243713379)\n",
      "(189, 4.2807087898254395)\n",
      "(190, 32.872188568115234)\n",
      "(191, 9.243881225585938)\n",
      "(192, 3.7812092304229736)\n",
      "(193, 12.888825416564941)\n",
      "(194, 3.712254285812378)\n",
      "(195, 3.128129243850708)\n",
      "(196, 12.28993034362793)\n",
      "(197, 11.516990661621094)\n",
      "(198, 3.317802667617798)\n",
      "(199, 8.597835540771484)\n",
      "(200, 3.2577927112579346)\n",
      "(201, 8.826844215393066)\n",
      "(202, 11.941567420959473)\n",
      "(203, 7.002533912658691)\n",
      "(204, 2.7707877159118652)\n",
      "(205, 9.41595458984375)\n",
      "(206, 4.877631664276123)\n",
      "(207, 1.9408838748931885)\n",
      "(208, 31.77052116394043)\n",
      "(209, 41.14938735961914)\n",
      "(210, 5.387420177459717)\n",
      "(211, 3.3617799282073975)\n",
      "(212, 19.212688446044922)\n",
      "(213, 10.6519775390625)\n",
      "(214, 65.40316772460938)\n",
      "(215, 3.273851156234741)\n",
      "(216, 15.033048629760742)\n",
      "(217, 7.965999126434326)\n",
      "(218, 12.091863632202148)\n",
      "(219, 36.18007278442383)\n",
      "(220, 24.638681411743164)\n",
      "(221, 26.693754196166992)\n",
      "(222, 5.127519130706787)\n",
      "(223, 11.451919555664062)\n",
      "(224, 10.076658248901367)\n",
      "(225, 32.53258514404297)\n",
      "(226, 15.345454216003418)\n",
      "(227, 7.821570873260498)\n",
      "(228, 2.70206880569458)\n",
      "(229, 9.24176025390625)\n",
      "(230, 15.55514144897461)\n",
      "(231, 4.590120792388916)\n",
      "(232, 4.252034664154053)\n",
      "(233, 6.255505084991455)\n",
      "(234, 5.296658039093018)\n",
      "(235, 31.382858276367188)\n",
      "(236, 6.735710620880127)\n",
      "(237, 3.9207119941711426)\n",
      "(238, 8.7168550491333)\n",
      "(239, 14.524049758911133)\n",
      "(240, 7.849031448364258)\n",
      "(241, 7.651340961456299)\n",
      "(242, 4.932779788970947)\n",
      "(243, 1.819293737411499)\n",
      "(244, 10.354758262634277)\n",
      "(245, 4.7252655029296875)\n",
      "(246, 2.052943229675293)\n",
      "(247, 5.12918758392334)\n",
      "(248, 1.1668999195098877)\n",
      "(249, 3.8547251224517822)\n",
      "(250, 2.977581262588501)\n",
      "(251, 1.1835997104644775)\n",
      "(252, 3.6703879833221436)\n",
      "(253, 1.2273379564285278)\n",
      "(254, 5.884159088134766)\n",
      "(255, 0.6664785742759705)\n",
      "(256, 4.006213188171387)\n",
      "(257, 1.379813313484192)\n",
      "(258, 3.418377161026001)\n",
      "(259, 4.383401870727539)\n",
      "(260, 1.699147343635559)\n",
      "(261, 2.2659621238708496)\n",
      "(262, 1.375575065612793)\n",
      "(263, 0.7007966637611389)\n",
      "(264, 4.5664963722229)\n",
      "(265, 1.7661677598953247)\n",
      "(266, 1.7684307098388672)\n",
      "(267, 4.405852794647217)\n",
      "(268, 0.7279140949249268)\n",
      "(269, 3.390834331512451)\n",
      "(270, 1.5158590078353882)\n",
      "(271, 1.6908338069915771)\n",
      "(272, 1.769068956375122)\n",
      "(273, 3.176198959350586)\n",
      "(274, 1.1081035137176514)\n",
      "(275, 1.556700348854065)\n",
      "(276, 1.8575783967971802)\n",
      "(277, 0.6886173486709595)\n",
      "(278, 1.0177844762802124)\n",
      "(279, 0.4605286419391632)\n",
      "(280, 2.7459676265716553)\n",
      "(281, 0.6567787528038025)\n",
      "(282, 1.4446275234222412)\n",
      "(283, 1.4092801809310913)\n",
      "(284, 1.2860773801803589)\n",
      "(285, 0.7310540676116943)\n",
      "(286, 0.3905162215232849)\n",
      "(287, 0.8445135354995728)\n",
      "(288, 0.46577778458595276)\n",
      "(289, 0.4445126950740814)\n",
      "(290, 2.673168182373047)\n",
      "(291, 0.8307353258132935)\n",
      "(292, 0.8169809579849243)\n",
      "(293, 0.7699066400527954)\n",
      "(294, 0.8824896216392517)\n",
      "(295, 0.2832348048686981)\n",
      "(296, 0.28141236305236816)\n",
      "(297, 0.5648655891418457)\n",
      "(298, 2.6108696460723877)\n",
      "(299, 2.0814971923828125)\n",
      "(300, 0.2939268946647644)\n",
      "(301, 1.2502530813217163)\n",
      "(302, 2.1469221115112305)\n",
      "(303, 1.2338638305664062)\n",
      "(304, 1.14927339553833)\n",
      "(305, 1.111899971961975)\n",
      "(306, 1.0681028366088867)\n",
      "(307, 0.2674824595451355)\n",
      "(308, 0.8869094848632812)\n",
      "(309, 0.3161061406135559)\n",
      "(310, 3.302072763442993)\n",
      "(311, 1.9143284559249878)\n",
      "(312, 1.8866747617721558)\n",
      "(313, 2.3072457313537598)\n",
      "(314, 1.1449781656265259)\n",
      "(315, 0.8223384618759155)\n",
      "(316, 2.5104238986968994)\n",
      "(317, 1.9129759073257446)\n",
      "(318, 0.8663884401321411)\n",
      "(319, 2.295213460922241)\n",
      "(320, 0.9582499861717224)\n",
      "(321, 0.8996331095695496)\n",
      "(322, 1.7906087636947632)\n",
      "(323, 0.7657983303070068)\n",
      "(324, 1.1070290803909302)\n",
      "(325, 0.7649610042572021)\n",
      "(326, 1.2499445676803589)\n",
      "(327, 0.5611680746078491)\n",
      "(328, 0.4460999369621277)\n",
      "(329, 0.39957195520401)\n",
      "(330, 2.0435469150543213)\n",
      "(331, 0.4022524952888489)\n",
      "(332, 0.4619440734386444)\n",
      "(333, 1.2233846187591553)\n",
      "(334, 2.074557065963745)\n",
      "(335, 0.5846600532531738)\n",
      "(336, 0.4460184872150421)\n",
      "(337, 0.6621993780136108)\n",
      "(338, 1.8466801643371582)\n",
      "(339, 0.3392484784126282)\n",
      "(340, 0.2706068158149719)\n",
      "(341, 0.42728468775749207)\n",
      "(342, 0.7064578533172607)\n",
      "(343, 0.6617989540100098)\n",
      "(344, 1.2105194330215454)\n",
      "(345, 0.3294302821159363)\n",
      "(346, 1.6466948986053467)\n",
      "(347, 0.2804473638534546)\n",
      "(348, 0.8439684510231018)\n",
      "(349, 1.652047872543335)\n",
      "(350, 0.2644757032394409)\n",
      "(351, 0.27807241678237915)\n",
      "(352, 0.3463951647281647)\n",
      "(353, 0.3876895010471344)\n",
      "(354, 0.47183123230934143)\n",
      "(355, 5.101357936859131)\n",
      "(356, 1.1748162508010864)\n",
      "(357, 3.978139638900757)\n",
      "(358, 3.40366530418396)\n",
      "(359, 0.6090399622917175)\n",
      "(360, 0.5225369930267334)\n",
      "(361, 1.4827693700790405)\n",
      "(362, 9.378652572631836)\n",
      "(363, 12.112748146057129)\n",
      "(364, 2.0667965412139893)\n",
      "(365, 3.4152426719665527)\n",
      "(366, 50.3681640625)\n",
      "(367, 8.049034118652344)\n",
      "(368, 6.717798233032227)\n",
      "(369, 26.468643188476562)\n",
      "(370, 63.430206298828125)\n",
      "(371, 2.4851748943328857)\n",
      "(372, 2.8012664318084717)\n",
      "(373, 49.99043273925781)\n",
      "(374, 83.02751159667969)\n",
      "(375, 6.683858871459961)\n",
      "(376, 18.390817642211914)\n",
      "(377, 11.132050514221191)\n",
      "(378, 36.98961639404297)\n",
      "(379, 10.707435607910156)\n",
      "(380, 12.170833587646484)\n",
      "(381, 15.291128158569336)\n",
      "(382, 2.2723677158355713)\n",
      "(383, 6.5142059326171875)\n",
      "(384, 7.042363166809082)\n",
      "(385, 15.352417945861816)\n",
      "(386, 12.594247817993164)\n",
      "(387, 7.733310222625732)\n",
      "(388, 6.580114364624023)\n",
      "(389, 3.2532474994659424)\n",
      "(390, 2.306467294692993)\n",
      "(391, 3.0228145122528076)\n",
      "(392, 34.876033782958984)\n",
      "(393, 3.531424045562744)\n",
      "(394, 2.713350772857666)\n",
      "(395, 9.174708366394043)\n",
      "(396, 2.782285213470459)\n",
      "(397, 20.142013549804688)\n",
      "(398, 2.1634068489074707)\n",
      "(399, 2.3031980991363525)\n",
      "(400, 3.269192934036255)\n",
      "(401, 1.970794439315796)\n",
      "(402, 1.6396090984344482)\n",
      "(403, 8.054753303527832)\n",
      "(404, 6.049520492553711)\n",
      "(405, 4.772777080535889)\n",
      "(406, 8.12883472442627)\n",
      "(407, 3.112025737762451)\n",
      "(408, 2.388763666152954)\n",
      "(409, 2.2973275184631348)\n",
      "(410, 1.6099761724472046)\n",
      "(411, 3.226165771484375)\n",
      "(412, 4.9678425788879395)\n",
      "(413, 1.7006689310073853)\n",
      "(414, 1.2661786079406738)\n",
      "(415, 1.4679104089736938)\n",
      "(416, 7.227942943572998)\n",
      "(417, 2.3502449989318848)\n",
      "(418, 3.0860278606414795)\n",
      "(419, 1.5454870462417603)\n",
      "(420, 2.5486128330230713)\n",
      "(421, 2.4555182456970215)\n",
      "(422, 3.8337490558624268)\n",
      "(423, 2.2218995094299316)\n",
      "(424, 1.345708966255188)\n",
      "(425, 0.80057692527771)\n",
      "(426, 0.7460879683494568)\n",
      "(427, 1.0147972106933594)\n",
      "(428, 1.2840946912765503)\n",
      "(429, 18.3757381439209)\n",
      "(430, 5.263274192810059)\n",
      "(431, 1.7704697847366333)\n",
      "(432, 1.579150915145874)\n",
      "(433, 6.1231207847595215)\n",
      "(434, 43.8782958984375)\n",
      "(435, 7.707033157348633)\n",
      "(436, 2.7133913040161133)\n",
      "(437, 9.541016578674316)\n",
      "(438, 5.015773296356201)\n",
      "(439, 15.847254753112793)\n",
      "(440, 18.040481567382812)\n",
      "(441, 6.723209857940674)\n",
      "(442, 4.551131248474121)\n",
      "(443, 2.3722476959228516)\n",
      "(444, 5.049502372741699)\n",
      "(445, 9.0685453414917)\n",
      "(446, 6.764862060546875)\n",
      "(447, 24.374588012695312)\n",
      "(448, 1.5266311168670654)\n",
      "(449, 2.690072774887085)\n",
      "(450, 4.143924713134766)\n",
      "(451, 11.967999458312988)\n",
      "(452, 8.560361862182617)\n",
      "(453, 2.721456289291382)\n",
      "(454, 2.0926592350006104)\n",
      "(455, 5.424084663391113)\n",
      "(456, 0.9396955370903015)\n",
      "(457, 27.9433650970459)\n",
      "(458, 12.355778694152832)\n",
      "(459, 1.8067879676818848)\n",
      "(460, 5.872451305389404)\n",
      "(461, 17.4161434173584)\n",
      "(462, 11.273226737976074)\n",
      "(463, 14.01189136505127)\n",
      "(464, 5.541952610015869)\n",
      "(465, 1.7608226537704468)\n",
      "(466, 5.140328407287598)\n",
      "(467, 9.305614471435547)\n",
      "(468, 9.151093482971191)\n",
      "(469, 9.84097671508789)\n",
      "(470, 3.0839030742645264)\n",
      "(471, 4.350353240966797)\n",
      "(472, 1.2876113653182983)\n",
      "(473, 3.688032627105713)\n",
      "(474, 2.6493659019470215)\n",
      "(475, 17.524566650390625)\n",
      "(476, 2.117229700088501)\n",
      "(477, 2.344770669937134)\n",
      "(478, 6.046295642852783)\n",
      "(479, 7.503556728363037)\n",
      "(480, 6.612921714782715)\n",
      "(481, 6.137715816497803)\n",
      "(482, 1.245251178741455)\n",
      "(483, 1.6702581644058228)\n",
      "(484, 2.7573604583740234)\n",
      "(485, 3.213329553604126)\n",
      "(486, 5.031131744384766)\n",
      "(487, 2.884208917617798)\n",
      "(488, 1.4004398584365845)\n",
      "(489, 1.588423728942871)\n",
      "(490, 2.7425057888031006)\n",
      "(491, 1.0652769804000854)\n",
      "(492, 8.004151344299316)\n",
      "(493, 3.885817527770996)\n",
      "(494, 1.7529566287994385)\n",
      "(495, 2.2081949710845947)\n",
      "(496, 4.921955585479736)\n",
      "(497, 3.08436918258667)\n",
      "(498, 2.180450201034546)\n",
      "(499, 0.8833784461021423)\n"
     ]
    }
   ],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "  def __init__(self, D_in, H, D_out):\n",
    "    \"\"\"\n",
    "    In the constructor we construct three nn.Linear instances that we will use\n",
    "    in the forward pass.\n",
    "    \"\"\"\n",
    "    super(DynamicNet, self).__init__()\n",
    "    self.input_linear = torch.nn.Linear(D_in, H)\n",
    "    self.middle_linear = torch.nn.Linear(H, H)\n",
    "    self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "    and reuse the middle_linear Module that many times to compute hidden layer\n",
    "    representations.\n",
    "\n",
    "    Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "    Python control-flow operators like loops or conditional statements when\n",
    "    defining the forward pass of the model.\n",
    "\n",
    "    Here we also see that it is perfectly safe to reuse the same Module many\n",
    "    times when defining a computational graph. This is a big improvement from Lua\n",
    "    Torch, where each Module could be used only once.\n",
    "    \"\"\"\n",
    "    h_relu = self.input_linear(x).clamp(min=0)\n",
    "    for _ in range(random.randint(0, 3)):\n",
    "      h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "    y_pred = self.output_linear(h_relu)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "  # Forward pass: Compute predicted y by passing x to the model\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # Compute and print loss\n",
    "  loss = criterion(y_pred, y)\n",
    "  print(t, loss.data[0])\n",
    "\n",
    "  # Zero gradients, perform a backward pass, and update the weights.\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScipyConv2dFunction(Function):\n",
    "\n",
    "    def forward(self, input, filter):\n",
    "        result = correlate2d(input.numpy(), filter.numpy(), mode='valid')\n",
    "        self.save_for_backward(input, filter)\n",
    "        return torch.FloatTensor(result)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        input, filter = self.saved_tensors\n",
    "        grad_input = convolve2d(grad_output.numpy(), filter.t().numpy(), mode='full')\n",
    "        grad_filter = convolve2d(input.numpy(), grad_output.numpy(), mode='valid')\n",
    "        return torch.FloatTensor(grad_input), torch.FloatTensor(grad_filter)\n",
    "\n",
    "\n",
    "class ScipyConv2d(Module):\n",
    "\n",
    "    def __init__(self, kh, kw):\n",
    "        super(ScipyConv2d, self).__init__()\n",
    "        self.filter = Parameter(torch.randn(kh, kw))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return ScipyConv2dFunction()(input, self.filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "module = ScipyConv2d(3, 3)\n",
    "print(list(module.parameters()))\n",
    "input = Variable(torch.randn(10, 10), requires_grad=True)\n",
    "output = module(input)\n",
    "print(output)\n",
    "output.backward(torch.randn(8, 8))\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
