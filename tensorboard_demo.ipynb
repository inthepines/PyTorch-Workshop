{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instatiating tensorboard writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 677.1868286132812)\n",
      "(1, 678.1227416992188)\n",
      "(2, 676.561767578125)\n",
      "(3, 698.6341552734375)\n",
      "(4, 671.9229125976562)\n",
      "(5, 670.2515869140625)\n",
      "(6, 573.4230346679688)\n",
      "(7, 652.8018188476562)\n",
      "(8, 467.53564453125)\n",
      "(9, 409.5158386230469)\n",
      "(10, 623.6585693359375)\n",
      "(11, 301.4396057128906)\n",
      "(12, 659.091552734375)\n",
      "(13, 655.9367065429688)\n",
      "(14, 583.3419189453125)\n",
      "(15, 656.2310791015625)\n",
      "(16, 645.7720336914062)\n",
      "(17, 530.07861328125)\n",
      "(18, 140.73728942871094)\n",
      "(19, 645.3768920898438)\n",
      "(20, 451.0494689941406)\n",
      "(21, 102.75556182861328)\n",
      "(22, 583.298095703125)\n",
      "(23, 564.9273071289062)\n",
      "(24, 539.439208984375)\n",
      "(25, 589.3126220703125)\n",
      "(26, 470.9310607910156)\n",
      "(27, 130.41708374023438)\n",
      "(28, 260.17230224609375)\n",
      "(29, 237.04893493652344)\n",
      "(30, 347.32928466796875)\n",
      "(31, 193.80763244628906)\n",
      "(32, 171.730224609375)\n",
      "(33, 257.2962951660156)\n",
      "(34, 332.11798095703125)\n",
      "(35, 197.78152465820312)\n",
      "(36, 140.16236877441406)\n",
      "(37, 228.80674743652344)\n",
      "(38, 134.16421508789062)\n",
      "(39, 215.24569702148438)\n",
      "(40, 188.85292053222656)\n",
      "(41, 156.29446411132812)\n",
      "(42, 152.92022705078125)\n",
      "(43, 127.78485870361328)\n",
      "(44, 262.9262390136719)\n",
      "(45, 149.23837280273438)\n",
      "(46, 165.82859802246094)\n",
      "(47, 109.68707275390625)\n",
      "(48, 92.81198120117188)\n",
      "(49, 39.720970153808594)\n",
      "(50, 70.8050537109375)\n",
      "(51, 127.0985107421875)\n",
      "(52, 185.1667938232422)\n",
      "(53, 122.3833236694336)\n",
      "(54, 110.26935577392578)\n",
      "(55, 168.53831481933594)\n",
      "(56, 67.65984344482422)\n",
      "(57, 83.86676025390625)\n",
      "(58, 150.63104248046875)\n",
      "(59, 178.0799102783203)\n",
      "(60, 65.183837890625)\n",
      "(61, 158.23731994628906)\n",
      "(62, 212.2039031982422)\n",
      "(63, 38.40270233154297)\n",
      "(64, 104.8966293334961)\n",
      "(65, 172.47727966308594)\n",
      "(66, 91.98418426513672)\n",
      "(67, 72.18804931640625)\n",
      "(68, 93.17569732666016)\n",
      "(69, 81.96373748779297)\n",
      "(70, 36.29159164428711)\n",
      "(71, 70.22101593017578)\n",
      "(72, 76.65360260009766)\n",
      "(73, 34.89484405517578)\n",
      "(74, 37.675697326660156)\n",
      "(75, 37.57225036621094)\n",
      "(76, 33.135379791259766)\n",
      "(77, 39.60466003417969)\n",
      "(78, 31.917203903198242)\n",
      "(79, 23.805011749267578)\n",
      "(80, 22.295185089111328)\n",
      "(81, 30.619020462036133)\n",
      "(82, 31.31793975830078)\n",
      "(83, 14.005744934082031)\n",
      "(84, 17.26541519165039)\n",
      "(85, 19.746421813964844)\n",
      "(86, 13.804434776306152)\n",
      "(87, 20.07168197631836)\n",
      "(88, 10.445052146911621)\n",
      "(89, 15.042694091796875)\n",
      "(90, 15.465019226074219)\n",
      "(91, 12.622868537902832)\n",
      "(92, 12.253362655639648)\n",
      "(93, 10.62630558013916)\n",
      "(94, 14.015707015991211)\n",
      "(95, 48.76419448852539)\n",
      "(96, 15.567312240600586)\n",
      "(97, 12.607741355895996)\n",
      "(98, 14.453566551208496)\n",
      "(99, 10.783153533935547)\n",
      "(100, 10.897966384887695)\n",
      "(101, 9.210943222045898)\n",
      "(102, 9.014359474182129)\n",
      "(103, 9.703899383544922)\n",
      "(104, 7.496661186218262)\n",
      "(105, 25.267948150634766)\n",
      "(106, 5.627712249755859)\n",
      "(107, 19.042579650878906)\n",
      "(108, 7.592226982116699)\n",
      "(109, 9.01456356048584)\n",
      "(110, 15.664875984191895)\n",
      "(111, 5.403409957885742)\n",
      "(112, 4.643100261688232)\n",
      "(113, 18.08414649963379)\n",
      "(114, 3.2285895347595215)\n",
      "(115, 6.992674827575684)\n",
      "(116, 9.843708038330078)\n",
      "(117, 6.244435787200928)\n",
      "(118, 2.7073886394500732)\n",
      "(119, 6.452747821807861)\n",
      "(120, 7.053130149841309)\n",
      "(121, 4.211893081665039)\n",
      "(122, 3.7100019454956055)\n",
      "(123, 4.4205803871154785)\n",
      "(124, 3.3556969165802)\n",
      "(125, 10.591384887695312)\n",
      "(126, 2.9170925617218018)\n",
      "(127, 9.4678373336792)\n",
      "(128, 3.7113497257232666)\n",
      "(129, 7.655081272125244)\n",
      "(130, 2.1894659996032715)\n",
      "(131, 1.9898197650909424)\n",
      "(132, 1.7574334144592285)\n",
      "(133, 9.665266990661621)\n",
      "(134, 3.904913902282715)\n",
      "(135, 3.657593011856079)\n",
      "(136, 1.7140467166900635)\n",
      "(137, 4.976675987243652)\n",
      "(138, 4.151987552642822)\n",
      "(139, 2.190840005874634)\n",
      "(140, 1.5752661228179932)\n",
      "(141, 2.7078652381896973)\n",
      "(142, 7.787066459655762)\n",
      "(143, 4.236413478851318)\n",
      "(144, 6.13693904876709)\n",
      "(145, 5.748264312744141)\n",
      "(146, 1.8284515142440796)\n",
      "(147, 4.67954683303833)\n",
      "(148, 2.4486746788024902)\n",
      "(149, 2.4257471561431885)\n",
      "(150, 3.914449453353882)\n",
      "(151, 1.5991891622543335)\n",
      "(152, 3.120810031890869)\n",
      "(153, 1.3519917726516724)\n",
      "(154, 1.318129539489746)\n",
      "(155, 8.306427955627441)\n",
      "(156, 0.9446980953216553)\n",
      "(157, 1.7970843315124512)\n",
      "(158, 3.8973991870880127)\n",
      "(159, 0.9484195113182068)\n",
      "(160, 0.7512850165367126)\n",
      "(161, 2.5476369857788086)\n",
      "(162, 1.3599090576171875)\n",
      "(163, 3.2550806999206543)\n",
      "(164, 3.2062783241271973)\n",
      "(165, 3.926530122756958)\n",
      "(166, 4.182372570037842)\n",
      "(167, 1.357986330986023)\n",
      "(168, 1.280639886856079)\n",
      "(169, 2.8412880897521973)\n",
      "(170, 3.4507932662963867)\n",
      "(171, 6.772224426269531)\n",
      "(172, 8.030288696289062)\n",
      "(173, 5.394669532775879)\n",
      "(174, 11.552377700805664)\n",
      "(175, 4.738347053527832)\n",
      "(176, 7.83844518661499)\n",
      "(177, 1.4135611057281494)\n",
      "(178, 3.69026517868042)\n",
      "(179, 6.7423248291015625)\n",
      "(180, 6.661365985870361)\n",
      "(181, 15.943583488464355)\n",
      "(182, 4.080417633056641)\n",
      "(183, 2.9740097522735596)\n",
      "(184, 6.716339588165283)\n",
      "(185, 18.346120834350586)\n",
      "(186, 6.0371294021606445)\n",
      "(187, 5.2416605949401855)\n",
      "(188, 37.27116012573242)\n",
      "(189, 4.844654560089111)\n",
      "(190, 7.686454772949219)\n",
      "(191, 15.65688419342041)\n",
      "(192, 8.38907527923584)\n",
      "(193, 3.3970589637756348)\n",
      "(194, 14.331631660461426)\n",
      "(195, 14.369407653808594)\n",
      "(196, 4.639023303985596)\n",
      "(197, 4.807577610015869)\n",
      "(198, 11.184355735778809)\n",
      "(199, 9.653512954711914)\n",
      "(200, 6.01127815246582)\n",
      "(201, 3.1185812950134277)\n",
      "(202, 3.307077169418335)\n",
      "(203, 1.093864917755127)\n",
      "(204, 6.229724407196045)\n",
      "(205, 62.436058044433594)\n",
      "(206, 3.939706802368164)\n",
      "(207, 34.235557556152344)\n",
      "(208, 9.804230690002441)\n",
      "(209, 61.312355041503906)\n",
      "(210, 17.62909507751465)\n",
      "(211, 7.396911144256592)\n",
      "(212, 40.87496566772461)\n",
      "(213, 32.2477912902832)\n",
      "(214, 18.359817504882812)\n",
      "(215, 13.731295585632324)\n",
      "(216, 5.191091537475586)\n",
      "(217, 17.958660125732422)\n",
      "(218, 11.874415397644043)\n",
      "(219, 19.151514053344727)\n",
      "(220, 16.33945083618164)\n",
      "(221, 32.64179992675781)\n",
      "(222, 5.781432151794434)\n",
      "(223, 13.73223876953125)\n",
      "(224, 5.543599605560303)\n",
      "(225, 18.956436157226562)\n",
      "(226, 2.633007049560547)\n",
      "(227, 5.537879467010498)\n",
      "(228, 5.606739521026611)\n",
      "(229, 5.019771575927734)\n",
      "(230, 5.322946548461914)\n",
      "(231, 13.78309440612793)\n",
      "(232, 2.5643863677978516)\n",
      "(233, 3.2940831184387207)\n",
      "(234, 3.7092559337615967)\n",
      "(235, 9.623932838439941)\n",
      "(236, 2.819622039794922)\n",
      "(237, 3.0299947261810303)\n",
      "(238, 13.074542045593262)\n",
      "(239, 2.5740864276885986)\n",
      "(240, 2.476806163787842)\n",
      "(241, 6.953544616699219)\n",
      "(242, 5.729523181915283)\n",
      "(243, 6.121119499206543)\n",
      "(244, 3.2363831996917725)\n",
      "(245, 5.626452445983887)\n",
      "(246, 5.418766975402832)\n",
      "(247, 12.620481491088867)\n",
      "(248, 1.3736763000488281)\n",
      "(249, 3.429692268371582)\n",
      "(250, 15.173537254333496)\n",
      "(251, 5.406126499176025)\n",
      "(252, 6.130431652069092)\n",
      "(253, 1.117035150527954)\n",
      "(254, 3.8404970169067383)\n",
      "(255, 4.081223011016846)\n",
      "(256, 10.352779388427734)\n",
      "(257, 8.570985794067383)\n",
      "(258, 4.761606693267822)\n",
      "(259, 18.135591506958008)\n",
      "(260, 17.99173355102539)\n",
      "(261, 9.38713264465332)\n",
      "(262, 2.5396971702575684)\n",
      "(263, 3.9298927783966064)\n",
      "(264, 9.337769508361816)\n",
      "(265, 24.823442459106445)\n",
      "(266, 5.783132076263428)\n",
      "(267, 13.50179672241211)\n",
      "(268, 4.758221626281738)\n",
      "(269, 27.816160202026367)\n",
      "(270, 2.29382586479187)\n",
      "(271, 7.622678279876709)\n",
      "(272, 3.701071262359619)\n",
      "(273, 9.151504516601562)\n",
      "(274, 7.107043743133545)\n",
      "(275, 11.276511192321777)\n",
      "(276, 2.5644800662994385)\n",
      "(277, 8.04375171661377)\n",
      "(278, 12.448580741882324)\n",
      "(279, 8.02598762512207)\n",
      "(280, 9.63294506072998)\n",
      "(281, 1.8318207263946533)\n",
      "(282, 1.9825795888900757)\n",
      "(283, 6.9418158531188965)\n",
      "(284, 7.260683059692383)\n",
      "(285, 5.57782506942749)\n",
      "(286, 7.457953453063965)\n",
      "(287, 9.236349105834961)\n",
      "(288, 1.927847146987915)\n",
      "(289, 0.8407266139984131)\n",
      "(290, 3.8137550354003906)\n",
      "(291, 4.014522075653076)\n",
      "(292, 3.2159976959228516)\n",
      "(293, 0.7971211075782776)\n",
      "(294, 2.7932159900665283)\n",
      "(295, 0.9810875654220581)\n",
      "(296, 0.9469726085662842)\n",
      "(297, 7.03303337097168)\n",
      "(298, 0.4763249456882477)\n",
      "(299, 0.7544487714767456)\n",
      "(300, 3.6094164848327637)\n",
      "(301, 1.4859031438827515)\n",
      "(302, 2.228905200958252)\n",
      "(303, 4.226428031921387)\n",
      "(304, 2.4833710193634033)\n",
      "(305, 0.7761268615722656)\n",
      "(306, 2.4891738891601562)\n",
      "(307, 1.5463017225265503)\n",
      "(308, 3.862680673599243)\n",
      "(309, 1.1879701614379883)\n",
      "(310, 1.2317200899124146)\n",
      "(311, 5.179861068725586)\n",
      "(312, 1.7670435905456543)\n",
      "(313, 0.938313901424408)\n",
      "(314, 1.6094019412994385)\n",
      "(315, 1.3976478576660156)\n",
      "(316, 1.1110671758651733)\n",
      "(317, 0.6493390202522278)\n",
      "(318, 1.590140700340271)\n",
      "(319, 2.320040464401245)\n",
      "(320, 0.9377643465995789)\n",
      "(321, 1.7200164794921875)\n",
      "(322, 2.5205883979797363)\n",
      "(323, 4.467418670654297)\n",
      "(324, 1.89884614944458)\n",
      "(325, 1.0508285760879517)\n",
      "(326, 0.5518788695335388)\n",
      "(327, 4.330077648162842)\n",
      "(328, 0.574343204498291)\n",
      "(329, 1.0630613565444946)\n",
      "(330, 0.5428486466407776)\n",
      "(331, 1.1270796060562134)\n",
      "(332, 2.784071445465088)\n",
      "(333, 1.7105125188827515)\n",
      "(334, 1.1239293813705444)\n",
      "(335, 1.5190647840499878)\n",
      "(336, 1.3595383167266846)\n",
      "(337, 0.9610615968704224)\n",
      "(338, 0.5674481391906738)\n",
      "(339, 1.5986919403076172)\n",
      "(340, 0.7874024510383606)\n",
      "(341, 0.5227614641189575)\n",
      "(342, 0.6156635880470276)\n",
      "(343, 1.0128004550933838)\n",
      "(344, 2.249340295791626)\n",
      "(345, 0.4448930621147156)\n",
      "(346, 0.8436837196350098)\n",
      "(347, 0.9087637662887573)\n",
      "(348, 0.30067458748817444)\n",
      "(349, 1.5050022602081299)\n",
      "(350, 0.2552732527256012)\n",
      "(351, 0.21128496527671814)\n",
      "(352, 0.9267321825027466)\n",
      "(353, 0.13340428471565247)\n",
      "(354, 2.226304054260254)\n",
      "(355, 0.6183384656906128)\n",
      "(356, 1.231918454170227)\n",
      "(357, 1.5640485286712646)\n",
      "(358, 0.5717191696166992)\n",
      "(359, 0.49610385298728943)\n",
      "(360, 0.905731737613678)\n",
      "(361, 0.8341381549835205)\n",
      "(362, 0.3098713159561157)\n",
      "(363, 0.3136644661426544)\n",
      "(364, 0.24193529784679413)\n",
      "(365, 0.8201900124549866)\n",
      "(366, 0.9309905767440796)\n",
      "(367, 1.30107581615448)\n",
      "(368, 0.4504091143608093)\n",
      "(369, 0.8584380149841309)\n",
      "(370, 0.9287741780281067)\n",
      "(371, 1.4631820917129517)\n",
      "(372, 1.1122804880142212)\n",
      "(373, 0.6566551327705383)\n",
      "(374, 0.7806188464164734)\n",
      "(375, 2.268512010574341)\n",
      "(376, 1.0812418460845947)\n",
      "(377, 0.9367750287055969)\n",
      "(378, 1.390761375427246)\n",
      "(379, 1.6614466905593872)\n",
      "(380, 0.707831084728241)\n",
      "(381, 0.8007169365882874)\n",
      "(382, 1.3764276504516602)\n",
      "(383, 0.19282667338848114)\n",
      "(384, 0.7298216223716736)\n",
      "(385, 0.6243962049484253)\n",
      "(386, 0.10982302576303482)\n",
      "(387, 0.8164262175559998)\n",
      "(388, 0.10272213816642761)\n",
      "(389, 1.357460856437683)\n",
      "(390, 0.1901547759771347)\n",
      "(391, 1.0231415033340454)\n",
      "(392, 0.2263524979352951)\n",
      "(393, 2.039041757583618)\n",
      "(394, 0.6319816708564758)\n",
      "(395, 1.0388400554656982)\n",
      "(396, 0.9719614386558533)\n",
      "(397, 0.5737714767456055)\n",
      "(398, 2.4356131553649902)\n",
      "(399, 0.6641237735748291)\n",
      "(400, 1.7515130043029785)\n",
      "(401, 1.1664656400680542)\n",
      "(402, 0.8484976291656494)\n",
      "(403, 0.1429327428340912)\n",
      "(404, 0.9502822160720825)\n",
      "(405, 0.555659294128418)\n",
      "(406, 0.46955782175064087)\n",
      "(407, 0.45276474952697754)\n",
      "(408, 0.9699474573135376)\n",
      "(409, 0.1338329166173935)\n",
      "(410, 0.4654077887535095)\n",
      "(411, 0.445884108543396)\n",
      "(412, 0.4049382507801056)\n",
      "(413, 0.48341354727745056)\n",
      "(414, 0.41704854369163513)\n",
      "(415, 0.5388904809951782)\n",
      "(416, 0.10555476695299149)\n",
      "(417, 0.38139647245407104)\n",
      "(418, 0.10251429677009583)\n",
      "(419, 1.1788607835769653)\n",
      "(420, 0.6475726962089539)\n",
      "(421, 0.2632628083229065)\n",
      "(422, 0.28782370686531067)\n",
      "(423, 0.08462008833885193)\n",
      "(424, 0.40581151843070984)\n",
      "(425, 1.6204944849014282)\n",
      "(426, 0.07712224125862122)\n",
      "(427, 0.3459709882736206)\n",
      "(428, 0.4897628128528595)\n",
      "(429, 0.3877352178096771)\n",
      "(430, 0.29503050446510315)\n",
      "(431, 0.3601151406764984)\n",
      "(432, 0.3142155706882477)\n",
      "(433, 0.3027642071247101)\n",
      "(434, 0.2385152131319046)\n",
      "(435, 0.24317538738250732)\n",
      "(436, 0.21201126277446747)\n",
      "(437, 0.21520762145519257)\n",
      "(438, 0.28174516558647156)\n",
      "(439, 1.6416149139404297)\n",
      "(440, 0.15597094595432281)\n",
      "(441, 0.24726249277591705)\n",
      "(442, 0.3741056025028229)\n",
      "(443, 0.13168583810329437)\n",
      "(444, 1.1550723314285278)\n",
      "(445, 0.20824190974235535)\n",
      "(446, 0.8855487704277039)\n",
      "(447, 0.15807144343852997)\n",
      "(448, 0.9295512437820435)\n",
      "(449, 0.15515539050102234)\n",
      "(450, 0.6104022860527039)\n",
      "(451, 0.3748687207698822)\n",
      "(452, 0.6270975470542908)\n",
      "(453, 0.5052394270896912)\n",
      "(454, 0.1299673616886139)\n",
      "(455, 0.13830670714378357)\n",
      "(456, 0.4565485119819641)\n",
      "(457, 0.11760605126619339)\n",
      "(458, 0.412932425737381)\n",
      "(459, 0.08260339498519897)\n",
      "(460, 0.7583286762237549)\n",
      "(461, 0.12345441430807114)\n",
      "(462, 0.1572984904050827)\n",
      "(463, 0.5758165717124939)\n",
      "(464, 0.14799079298973083)\n",
      "(465, 0.3193208575248718)\n",
      "(466, 0.5206950306892395)\n",
      "(467, 0.4688064455986023)\n",
      "(468, 0.4176836311817169)\n",
      "(469, 0.26629874110221863)\n",
      "(470, 0.1473228484392166)\n",
      "(471, 0.28149619698524475)\n",
      "(472, 0.2586636543273926)\n",
      "(473, 0.22837117314338684)\n",
      "(474, 0.6625876426696777)\n",
      "(475, 0.6147884130477905)\n",
      "(476, 0.4279946982860565)\n",
      "(477, 0.18199187517166138)\n",
      "(478, 0.4419823884963989)\n",
      "(479, 0.17675329744815826)\n",
      "(480, 0.6778571009635925)\n",
      "(481, 0.2359011173248291)\n",
      "(482, 0.18443910777568817)\n",
      "(483, 0.6441827416419983)\n",
      "(484, 0.35150349140167236)\n",
      "(485, 0.30584341287612915)\n",
      "(486, 0.09305821359157562)\n",
      "(487, 0.6573243737220764)\n",
      "(488, 0.3069581091403961)\n",
      "(489, 0.1013794094324112)\n",
      "(490, 0.31031692028045654)\n",
      "(491, 0.3363986015319824)\n",
      "(492, 0.27092891931533813)\n",
      "(493, 0.2839076817035675)\n",
      "(494, 0.23900462687015533)\n",
      "(495, 0.20423778891563416)\n",
      "(496, 0.08250664919614792)\n",
      "(497, 0.7881842851638794)\n",
      "(498, 0.07344029098749161)\n",
      "(499, 0.13637827336788177)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    writer.add_scalar('loss', loss.data.numpy()[0], t)\n",
    "    print(t, loss.data[0])\n",
    "    if t % 10 == 0:\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(name, param.clone().data.numpy(), t)\n",
    "\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "z = model(Variable(torch.randn(N, D_in), requires_grad=True))\n",
    "writer.add_graph(model, z)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## To see the tensorboard results do ` tensorboard --logdir runs`"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
