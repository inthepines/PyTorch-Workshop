{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-38bd2b36459f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-38bd2b36459f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    batch_size = # Your code here\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "batch_size = # Your code here\n",
    "test_batch_size = # Your code here\n",
    "epochs = # Your code here\n",
    "lr = # Your code here\n",
    "momentum = # Your code here\n",
    "cuda = # Your code here\n",
    "log_interval = # Your code here\n",
    "seed = # Your code here\n",
    "\n",
    "cuda = cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading  and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Model\n",
    "\n",
    "### Here we are going to be making a model with two convolutional layers.  The first of which has 1 input channel and 10 output channels. The second has 10 input channels and 20 output channels. Both have kernel size of 5\n",
    "\n",
    "### After that we apply two dimensional dropout\n",
    "\n",
    "### Then we have two linear layers. One with 320 inputs to 50 outputs. And one with 50 inputs to 10 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geting model ready to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299652\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.292955\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.285500\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.284651\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.217024\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.186156\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.154740\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.988293\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.895573\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.704662\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.422654\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.235478\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.143874\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.712348\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.873412\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.837864\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.907484\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.828504\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.627906\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.630847\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.633108\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.568438\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.790113\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.550007\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.615618\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.424209\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.358447\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.770360\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.458532\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.367244\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.413168\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.295271\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.315074\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.341451\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.431364\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.492369\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.367297\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.508948\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.376266\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.415233\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.381440\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.219856\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.450031\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.737023\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.382993\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.479736\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.373687\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.429145\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.248182\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.361417\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.257997\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.330156\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.132118\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.378354\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.252750\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.311023\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.310348\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.457771\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.270780\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.296766\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.383872\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.202003\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.450168\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.189089\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.295995\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.336869\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.374868\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.292943\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.229288\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.186189\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.383918\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.216348\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.478663\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.294687\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.417329\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.147660\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.137270\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.154812\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.286107\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.225076\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.105412\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.354865\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.197215\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.450316\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.352827\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.305315\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.382878\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.200069\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.139050\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.349296\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.381500\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.354287\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.291827\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.412384\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and other model attributes\n",
    "\n",
    "### One of the nice things about the way we have set up our model is we can directly play around with the attirbutes of it, including the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "(0 ,0 ,.,.) = \n",
       " -0.1102  0.2251  0.0109  0.0458 -0.0505\n",
       "  0.1718  0.1494 -0.0519 -0.0144 -0.0725\n",
       "  0.0690  0.1312 -0.0892  0.1011 -0.0129\n",
       " -0.0922 -0.1526 -0.0689  0.0680 -0.0512\n",
       " -0.0507 -0.0800 -0.1448  0.1241 -0.0863\n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       " -0.0659  0.1935 -0.0510  0.1051  0.1607\n",
       " -0.0453 -0.0518  0.1868  0.1732  0.1745\n",
       " -0.0406  0.0122  0.1188  0.2780  0.2496\n",
       " -0.0998 -0.1695 -0.0160  0.0955 -0.1066\n",
       "  0.1460  0.1668  0.0940  0.0812  0.1824\n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       "  0.0430 -0.0550  0.1221  0.0797  0.2412\n",
       " -0.1339 -0.1186  0.2462  0.3079  0.3693\n",
       " -0.0807  0.1536 -0.0126  0.3750  0.1822\n",
       "  0.0835  0.0684  0.2856  0.2242  0.0698\n",
       " -0.0954 -0.0023  0.1462  0.1489 -0.0635\n",
       "\n",
       "(3 ,0 ,.,.) = \n",
       " -0.0575  0.1601 -0.2158  0.1408 -0.1719\n",
       "  0.2074  0.2519  0.0000 -0.2113 -0.0792\n",
       "  0.0482  0.2219 -0.1730 -0.1864  0.0080\n",
       " -0.0781  0.1607 -0.1033  0.1062  0.2565\n",
       "  0.2274  0.2697  0.1217  0.2618  0.3001\n",
       "\n",
       "(4 ,0 ,.,.) = \n",
       " -0.0600 -0.1657  0.0597  0.3278  0.2908\n",
       "  0.0750  0.1582 -0.0312  0.3413  0.3290\n",
       " -0.1708  0.0221 -0.0386  0.0312  0.0258\n",
       " -0.2176 -0.0149  0.2329  0.1693 -0.1233\n",
       "  0.0166 -0.1824  0.1675  0.1647  0.0002\n",
       "\n",
       "(5 ,0 ,.,.) = \n",
       " -0.1632 -0.0777 -0.2085  0.0842  0.1512\n",
       " -0.1261 -0.0089 -0.2363 -0.1995  0.1685\n",
       "  0.0738  0.1719 -0.1369 -0.0783  0.1511\n",
       "  0.2760  0.2614  0.2826  0.1805  0.1343\n",
       "  0.0292 -0.0523  0.1909  0.3549  0.1623\n",
       "\n",
       "(6 ,0 ,.,.) = \n",
       "  0.3216  0.2362  0.1545  0.0750  0.0980\n",
       "  0.2966  0.2066  0.3031  0.1145 -0.0342\n",
       "  0.0096 -0.0425  0.1552  0.0872  0.1117\n",
       " -0.2225 -0.1154 -0.0067  0.1966 -0.1198\n",
       " -0.0408 -0.0714  0.0079 -0.0327 -0.1246\n",
       "\n",
       "(7 ,0 ,.,.) = \n",
       " -0.0369 -0.0588 -0.2032 -0.1259 -0.2004\n",
       "  0.0661 -0.1359 -0.0290 -0.0680 -0.0985\n",
       " -0.1546 -0.2089  0.0079 -0.1205  0.0932\n",
       " -0.2129 -0.0628  0.0755 -0.0443 -0.0873\n",
       "  0.1531  0.0506 -0.1734 -0.1978 -0.1128\n",
       "\n",
       "(8 ,0 ,.,.) = \n",
       "  0.0687 -0.0659  0.0593 -0.0526  0.2453\n",
       "  0.0196  0.0716 -0.0504  0.2650  0.1775\n",
       " -0.0303  0.0510  0.0248  0.1836 -0.1031\n",
       "  0.1465 -0.1575 -0.0828  0.2022  0.0084\n",
       "  0.0021 -0.1000 -0.1680 -0.1560  0.1528\n",
       "\n",
       "(9 ,0 ,.,.) = \n",
       "  0.0761  0.1456  0.0787  0.3397  0.4065\n",
       "  0.2781  0.2936  0.0410 -0.0508  0.0682\n",
       " -0.0307  0.1389  0.0743  0.2104 -0.0743\n",
       " -0.3094 -0.2781 -0.2726 -0.1354 -0.1632\n",
       " -0.1261 -0.2760 -0.2051 -0.2708 -0.0965\n",
       "[torch.FloatTensor of size 10x1x5x5]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
